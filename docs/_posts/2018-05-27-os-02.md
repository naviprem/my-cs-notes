---
layout: post
title:  "OS Part 01 - Threads"
date:   2018-05-27
categories: OS
---

Reference: Udacity's [Introduction to Operating System](https://classroom.udacity.com/courses/ud923)

### Single Threaded Process
* Can execute on only on CPT at a given point of time
* In order to take advantage of multi core CPUs, the process needs to have multiple execution contexts.

### Multi Threaded Process
* Has a more complex PCB data structure 
* One part of the PCB contains Code, Data and files that all threads share
* Another part of the PCB contains registers and stack for each thread

### Benefits
* A costlier operation during process context switch is virtual-physical address mapping. Since threads share the same address space, thread context switching can avoid this costly step. For this reason threads are useful even on a single CPU.
* Speed due to parallelization. e.g. Threads can partition the input array and process in parallel
* Speed due to specialization. Partitioning processes into small specialized tasks leads smaller data requirements on cache, which leads to hot cache. 
* Thread process switching time is less than process context switching time

### Cons
* Threads share the same virtual to physical address mapping. All threads in a process has access to the same physical memory and uses the same virtual adresses to access them.
* `Data Race:` Inconsistent state of data when 2 or more threads are trying to access the same variable

### Synchronization Mechanisms:
* `Mutex`: Mutual Exclusion is an operation where only one thread is allowed to perform an operation at a time. If multiple threads want to perform the same operation at the same time, they need to wait their turn.
* `Waiting:` Waiting on a specific condition
* `Notifying:` Notifying and waking up other threads from a wait state

### Thread Type:
* It is the data structure of the thread
* It contains, thread ID, Program Counter, Stack Pointer, registers, stack, attributes, ...

### Thread creation
Sudo code for thread t0 to create a new thread t1
```angular2html
t1 = fork(proc, args)

PC = proc
stack = args
```

When t1 completes its computation, it returns its results to the parent thread t0
```angular2html
child.result = join(t1)
```
When a parent thread calls join on a child thread, it is blocked until the child thread completes its computation and returns its results to the parent.

### Thread creation e.g.
```angular2html
Thread t1;
List[int] my_list;
t1 = fork(safe_insert, 4);
safe_insert(6);
join(t1);
```

### Mutexes
* Construct provided by the Operating System to avoid race conditions

### Mutex Data Structure
```angular2html
Bool isLocked;
Thread owner;
List[Thread] blocked_threads;
```

### Mutex API
```angular2html
lock(Mutex);
unlock(Mutex);
```

### Mutex e.g.
```angular2html
List[int] my_list;
Mutex m;
void safe_insert(int n) {
    lock(m);
    my_list.insert(n);
    unlock(m);
}
```

### Producer/Consumer problem
* What if the mutex operation needs to be performed on a specific condition?
* A consumer thread has to wait until the list gets full to print the list.
* Common situation in OS, buffer read/write
* Needs additional constructs other than Mutex
* Wait() and Notify() are used to handle such conditions
* The wait() method is passed 2 arguments, mutex and wait-condition variables
* The wait() method's implementation will release and acquire the mutex before and after the wait.

### Condition Variable API
```angular2html
Mutex m;
List[Thread] waiting_threads;
```

### Condition Variable API
```angular2html
wait(m, cond)
signal(cond)
broadcast(cond)

// Some implementations have a common notify() method in place of signal and broadcast.
```
### Wait method sudo code
```angular2html
wait(m, cond) {
    release mutex
    get into wait queue
    until notified
        wait
    when notified {
        remove from wait queue
        re-acquire mutex
    }
}
```
### Producer Consumer code
```angular2html
// consumer: print and clear
lock(m)
while(my_list.not_full())
    wait(m, list_full)
my_list.print_and_clear()
unlock(m)

// producer: safe insert
lock(m)
my_list.insert(n);
if my_list.full() 
    notify(list_full);
unlock(m)
```

### Reader/writer problem
* Some threads want to read a resource, some others what to write
* Multiple readers can read at the same time
* Only one thread can write at a time
* Cannot read when write operation is in progress.

### Reader/Write code
```angular2html
// Reader

lock(m)
while (ctr == -1)
    wait(m, can_read)
ctr++
unlock(m)
// perform read
lock(m)
ctr--
if (ctr == 0)
    notify(can_write)
unlock(m)

// Writer

lock(m)
while (ctr != 0)
    wait(m, can_write)
ctr = -1
unlock(m)
// perform write
lock(m)
ctr = 0
notify(can_read)
notify(can_write)
unlock(m)
```

### Pit-falls in multithreading applications
* `Spurious wake-ups:` does not affect correctness but will impact performance. when threads are woken up by broadcast() only one thread can actually acquire the mutex, the rest will have to go back to wait again
* `Deadlock:` If 2 threads are waiting for a resource the each other is holding to complete their respective operations. 
* Deadlocks can be detected by generating a graph where nodes are threads and edges are drawn from threads waiting on a resource to threads holding the resource.
* If a cycle is present in the graph then there is a deadlock.
* Deadlock prevention is expensive
* Deadlock detection and recovery involves rollback, which again is not performant
* hence do nothing!

### Multithreading patterns to structure applications that use multiple threads

1. Boss worker
2. Pipeline
3. Layered

### Boss worker pattern
* Boss assigns work by placing it on the producer consumer queue
* Boss does not need to know details about the workers
* queue synchronization helps sync the work
* if number of worker threads is less, boss thread will have idle times
* if number of worker threads is more, worker threads will be idle
* solution is to use a dynamic pool of threads, that can increase of decrease the number of workers in the pool based on demand
* this comes with overhead of adjusting the pool size
* no locality, all workers do all tasks, cannot optimize resources

### Pipeline pattern
* Overall tasks is divided into subtasks
* Each thread performs one task
* Use shared buffer/queue to share resources
* Use thread pooling, to address long running tasks 
* Locality provides resource optimization

### Layered pattern
* Layers of related sub-tasks
* threads are assigned any task within a layer
* has the benefits of specialization
* less fine grained than pipelines
* not suitable for all application
* Synchronization could get a lil complicated

### Interupts and Signal
* `Interrupts:` generated externally by components other than the CPU, like I/O device, timers, other CPUs,...
* `Signals:` occur when events like illegal memory access, divide by zero or process kill
* When a interrupt or signal happens, an interrupt or signal handler table is referenced to get the starting address of the handler and copied to program counter. Then the handler is executed in the context of the thread 

 
